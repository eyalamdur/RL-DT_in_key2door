import gymnasium as gym
# import utils
from tqdm import trange
from stable_baselines3 import PPO

MAX_EPISODE_STEPS = 30  # Maximum steps per episode for PPO in k2d environment

def train_ppo(env: gym.Env, num_episodes: int = 1000) -> PPO:
    """
    Train a PPO agent on the given environment.
    Args:
        env (gym.Env): The environment to train the agent on.
        num_episodes (int): The number of episodes to train the agent.
    Returns:
        agent (PPO): The trained PPO agent.
    """
    # Create the PPO agent
    model = PPO(
        policy="MultiInputPolicy",      # MultiInputPolicy is used for environments with multiple input spaces
        env=env,
        device="cpu",                   # Use CPU for training
        learning_rate=3e-4,             # Use schedule or tune between 1e-4 and 3e-4
        n_steps=4096,                   # Large enough for long-term planning
        batch_size=256,                 # Should divide n_steps evenly
        n_epochs=20,                    # More passes per update for thorough learning
        gamma=0.995,                    # High discount for long-term reward
        gae_lambda=0.97,                # Balanced bias-variance tradeoff
        ent_coef=0.001,                 # Encourage minimal exploration
        verbose=0
    )
    steps_per_episode = env.spec.max_episode_steps or MAX_EPISODE_STEPS
    
    # tqdm progress bar
    with trange(num_episodes, desc="Training PPO", unit="episode") as pbar:
        for _ in pbar:
            model.learn(total_timesteps=steps_per_episode, reset_num_timesteps=False)

    # model_id = utils.get_next_run_id("results/models/PPO", "models")
    model_id = "1"
    model.save(f"results/models/PPO/ppo_{model_id}")

    return model

def load_ppo(model_path: str) -> PPO:
    """
    Load a trained PPO agent from a file.
    Args:
        model_path (str): The path to the saved PPO model.
    Returns:
        agent (PPO): The loaded PPO agent.
    """
    model = PPO.load(model_path, device="cpu")
    return model